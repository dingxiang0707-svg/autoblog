#!/usr/bin/env python3
"""
McKinseyæ–‡ç« æŠ“å–APIæœåŠ¡å™¨
æä¾›HTTPæ¥å£ä¾›n8nè°ƒç”¨ï¼Œè¿”å›æ–‡ç« å†…å®¹
"""

import time
import json
import os
import zipfile
from datetime import datetime
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import traceback
import logging
from logging.handlers import RotatingFileHandler
from playwright.sync_api import sync_playwright

app = Flask(__name__)
CORS(app)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if not os.path.exists('logs'):
    os.makedirs('logs')
    
file_handler = RotatingFileHandler('logs/mckinsey_api.log', maxBytes=10240000, backupCount=10)
file_handler.setFormatter(logging.Formatter(
    '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
))
file_handler.setLevel(logging.INFO)
app.logger.addHandler(file_handler)

class McKinseyScraperAPI:
    def __init__(self):
        # APIæœåŠ¡å™¨çš„å·¥ä½œç›®å½•
        self.work_dir = os.path.abspath(os.path.dirname(__file__))
        self.static_dir = os.path.join(self.work_dir, 'static')
        self.files_dir = os.path.join(self.static_dir, 'files')
        
        # å†å²æ–‡ä»¶
        self.links_file = os.path.join(self.work_dir, 'latest_two_articles.json')
        
        # N8Nç›‘æ§çš„è¾“å‡ºæ–‡ä»¶å¤¹
        self.n8n_output_dir = os.path.join(self.static_dir, 'mckinsey_output')
        self.ensure_directories()
        
        self.base_url = "https://www.mckinsey.com/capabilities/quantumblack/our-insights"
    
    def ensure_directories(self):
        """ç¡®ä¿æ‰€éœ€ç›®å½•å­˜åœ¨"""
        for directory in [self.static_dir, self.files_dir, self.n8n_output_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)
                logger.info(f"âœ… åˆ›å»ºç›®å½•: {directory}")
    
    def clean_old_files(self, max_age_hours=24):
        """æ¸…ç†è¶…è¿‡æŒ‡å®šæ—¶é—´çš„æ—§æ–‡ä»¶"""
        try:
            current_time = time.time()
            max_age_seconds = max_age_hours * 3600
            
            for directory in [self.files_dir, self.n8n_output_dir]:
                for root, dirs, files in os.walk(directory):
                    for file in files:
                        file_path = os.path.join(root, file)
                        if current_time - os.path.getctime(file_path) > max_age_seconds:
                            try:
                                os.remove(file_path)
                                logger.info(f"ğŸ—‘ï¸ æ¸…ç†æ—§æ–‡ä»¶: {file_path}")
                            except Exception as e:
                                logger.error(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    
                    # æ¸…ç†ç©ºç›®å½•
                    for dir_name in dirs:
                        dir_path = os.path.join(root, dir_name)
                        try:
                            if not os.listdir(dir_path):
                                os.rmdir(dir_path)
                                logger.info(f"ğŸ—‘ï¸ æ¸…ç†ç©ºç›®å½•: {dir_path}")
                        except Exception:
                            pass
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def parse_date_for_sorting(self, date_str):
        """è§£ææ—¥æœŸç”¨äºæ’åº"""
        if not date_str or date_str == "æœªæ‰¾åˆ°æ—¶é—´":
            return datetime(1900, 1, 1)
        
        try:
            # å¤„ç† "September 10, 2025" æ ¼å¼
            return datetime.strptime(date_str, "%B %d, %Y")
        except:
            try:
                # å¤„ç†å…¶ä»–å¯èƒ½çš„æ ¼å¼
                return datetime.strptime(date_str, "%Y-%m-%d")
            except:
                return datetime(1900, 1, 1)

    def load_existing_articles(self):
        """åŠ è½½å·²å­˜åœ¨çš„æ–‡ç« URLå’Œæœ€æ–°æ—¥æœŸ"""
        existing_urls = set()
        latest_date = datetime(1900, 1, 1)  # åˆå§‹åŒ–ä¸ºå¾ˆä¹…ä¹‹å‰çš„æ—¥æœŸ
        all_historical_articles = []
        
        if os.path.exists(self.links_file):
            try:
                with open(self.links_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # è·å–å†å²æ–‡ç« åˆ—è¡¨
                    if 'latest_two_articles' in data:
                        all_historical_articles = data['latest_two_articles']
                    elif isinstance(data, list):
                        all_historical_articles = data
                    
                    # æ”¶é›†æ‰€æœ‰URLå¹¶æ‰¾åˆ°æœ€æ–°æ—¥æœŸ
                    for article in all_historical_articles:
                        existing_urls.add(article['url'])
                        article_date = self.parse_date_for_sorting(article.get('date', ''))
                        if article_date > latest_date:
                            latest_date = article_date
                            
                logger.info(f"ä» {self.links_file} åŠ è½½äº† {len(existing_urls)} ä¸ªå·²å­˜åœ¨çš„æ–‡ç« URL")
                logger.info(f"æœ€æ–°æ–‡ç« æ—¥æœŸ: {latest_date.strftime('%B %d, %Y') if latest_date.year > 1900 else 'æ— å†å²è®°å½•'}")
                
            except Exception as e:
                logger.error(f"åŠ è½½ {self.links_file} å¤±è´¥: {e}")
        else:
            logger.info("æœªæ‰¾åˆ°å†å²æ–‡ä»¶ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œ")
        
        return existing_urls, latest_date, all_historical_articles

    def is_valid_article(self, href, title, full_href):
        """
        æ”¹è¿›çš„æ–‡ç« éªŒè¯é€»è¾‘ - ç²¾ç¡®ç­›é€‰æ‰éæ–‡ç« é“¾æ¥
        """
        # åŸºæœ¬URLæ ¼å¼æ£€æŸ¥
        if not href or not title:
            return False
        
        # æ’é™¤æ˜æ˜¾çš„éæ–‡ç« é“¾æ¥
        excluded_patterns = [
            'app-store', 'play.google', 'apple.com',
            '/careers', '/contact-us', '/search',
            '/subscribe', '/newsletter', '/events', '/privacy',
            '/terms', '/cookie', '/accessibility',
            'linkedin.com', 'twitter.com', 'facebook.com'
        ]
        
        # æ£€æŸ¥URLæ’é™¤æ¨¡å¼
        for pattern in excluded_patterns:
            if pattern in full_href.lower():
                return False
        
        # å®šä¹‰éæ–‡ç« æ ‡é¢˜çš„é»‘åå•
        title_blacklist_exact = [
            'read the article', 'read more', 'learn more',
            'view article', 'see more', 'continue reading',
            'contact us', 'contact', 'scam warning', 'terms of use',
            'local language information', 'accessibility statement',
            'cookie notice', 'privacy notice', 'privacy policy',
            'more menu options', 'subscribe', 'newsletter',
            'more', 'menu', 'search', 'login', 'sign up', 'register',
            'home', 'about', 'careers'
        ]
        
        # éƒ¨åˆ†åŒ¹é…çš„é»‘åå•å…³é”®è¯
        title_blacklist_partial = [
            'contact us', 'scam warning', 'terms of use',
            'local language information', 'accessibility statement', 
            'cookie notice', 'privacy notice', 'more menu options',
            'subscribe', 'newsletter'
        ]
        
        title_lower = title.lower().strip()
        
        # æ£€æŸ¥å®Œæ•´åŒ¹é…
        if title_lower in title_blacklist_exact:
            return False
        
        # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…
        if any(blacklisted in title_lower for blacklisted in title_blacklist_partial):
            return False
        
        # æ’é™¤è¿‡äºç®€çŸ­æˆ–é€šç”¨çš„æ ‡é¢˜
        if len(title.split()) <= 2 and len(title.strip()) < 30:
            return False
        
        # æ’é™¤ä»¥çœç•¥å·ç»“å°¾çš„ç®€çŸ­æ ‡é¢˜
        if title.strip().endswith('...') and len(title.strip()) < 30:
            return False
        
        min_title_length = 15
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ç« ç›¸å…³è·¯å¾„
        article_indicators = [
            '/our-insights/', '/capabilities/', '/industries/', 
            '/featured-insights/', '/blog/', '/article/',
            '/about-us/new-at-mckinsey-blog/'
        ]
        
        has_article_path = any(indicator in full_href.lower() for indicator in article_indicators)
        
        # ç¡®ä¿URLæ®µè½è¶³å¤Ÿé•¿
        url_segments = href.split('/')
        last_segment = url_segments[-1] if url_segments else ""
        has_meaningful_url = len(last_segment) > 10
        
        # æœ€ç»ˆåˆ¤æ–­æ¡ä»¶
        is_valid = (
            len(title.strip()) >= min_title_length and
            has_article_path and
            has_meaningful_url and
            not title.lower().startswith(('http', 'www', 'click'))
        )
        
        return is_valid

    def extract_latest_articles(self):
        """æå–æœ€æ–°çš„æ–‡ç« é“¾æ¥ï¼ˆåªè¦æ¯”ä¸Šæ¬¡æ›´æ–°çš„ï¼‰"""
        logger.info("ğŸ” æ­¥éª¤1: æå–McKinseyæœ€æ–°æ–‡ç« é“¾æ¥...")
        
        # åŠ è½½å·²å­˜åœ¨çš„æ–‡ç« å’Œæœ€æ–°æ—¥æœŸ
        existing_urls, latest_date, all_historical_articles = self.load_existing_articles()
        logger.info(f"å·²å­˜åœ¨æ–‡ç« æ•°é‡: {len(existing_urls)}")
        
        with sync_playwright() as p:
            browser = p.firefox.launch(
                headless=True,
                firefox_user_prefs={
                    "dom.webdriver.enabled": False,
                    "useAutomationExtension": False,
                }
            )
            context = browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/131.0.0.0 Safari/537.36"
                ),
                viewport={"width": 1280, "height": 800},
                locale='en-US',
                timezone_id='America/New_York'
            )
            page = context.new_page()
            
            # éšè— webdriver ç‰¹å¾
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            try:
                logger.info(f"è®¿é—®é¡µé¢: {self.base_url}")
                
                page.goto(
                    self.base_url, 
                    wait_until="domcontentloaded",
                    timeout=60000  # 60ç§’è¶…æ—¶
                )
                time.sleep(8)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                
                logger.info(f"é¡µé¢æ ‡é¢˜: {page.title()}")
                
                # å°è¯•æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
                logger.info("æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹...")
                for i in range(3):
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(2)
                    logger.info(f"  æ»šåŠ¨ {i+1}/3...")
                
                # è·å–æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é“¾æ¥
                logger.info("è·å–æ–‡ç« é“¾æ¥...")
                
                link_selectors = [
                    "a[href*='/our-insights/']",
                    "a[href*='/capabilities/']", 
                    "a[href*='/industries/']",
                    "a[href*='/featured-insights/']",
                    "a[data-component='mdc-c-link']",
                    "a[href*='mckinsey.com']",
                    "a[class*='mdc-c-link']"
                ]
          
                all_articles = []
                
                for selector in link_selectors:
                    try:
                        links = page.query_selector_all(selector)
                        logger.info(f"é€‰æ‹©å™¨ '{selector}': æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                        
                        for link in links:
                            href = link.get_attribute("href")
                            title = link.inner_text().strip()
                            
                            if href and title:
                                # æ ‡å‡†åŒ–URL
                                if href.startswith("/"):
                                    full_href = "https://www.mckinsey.com" + href
                                else:
                                    full_href = href
                                
                                # ä½¿ç”¨æ”¹è¿›çš„ç­›é€‰é€»è¾‘
                                is_real_article = self.is_valid_article(href, title, full_href)
                                
                                if is_real_article:
                                    # é¿å…é‡å¤
                                    if not any(article['url'] == full_href for article in all_articles):
                                        all_articles.append({
                                            "title": title,
                                            "url": full_href,
                                            "date": None,
                                            "snippet": ""
                                        })
                                        logger.info(f"  å‘ç°æ–‡ç« : {title[:60]}...")
                            
                    except Exception as e:
                        logger.error(f"é€‰æ‹©å™¨ '{selector}' å¤±è´¥: {e}")
                
                logger.info(f"æ€»å…±æ‰¾åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")
                
                # é‡æ–°è·å–æ–‡ç« å’Œæ—¶é—´ä¿¡æ¯ï¼Œç¡®ä¿åŒ¹é…æ­£ç¡®
                logger.info("é‡æ–°è·å–æ–‡ç« å’Œæ—¶é—´ä¿¡æ¯ä»¥ç¡®ä¿æ­£ç¡®åŒ¹é…...")
                
                # æŸ¥æ‰¾æ–‡ç« å®¹å™¨ï¼Œä»å®¹å™¨ä¸­åŒæ—¶æå–æ–‡ç« å’Œæ—¶é—´
                article_containers = page.query_selector_all("[class*='GenericItem'], [data-component*='generic-item'], .mdc-c-generic-item")
                logger.info(f"æ‰¾åˆ° {len(article_containers)} ä¸ªæ–‡ç« å®¹å™¨")
                
                matched_articles = []
                
                for i, container in enumerate(article_containers):
                    try:
                        # åœ¨å®¹å™¨å†…æŸ¥æ‰¾é“¾æ¥
                        links = container.query_selector_all("a")
                        article_link = None
                        article_title = None
                        
                        for link in links:
                            href = link.get_attribute("href")
                            title = link.inner_text().strip()
                            
                            if href and title and len(title) > 10:
                                # æ ‡å‡†åŒ–URL
                                if href.startswith("/"):
                                    full_href = "https://www.mckinsey.com" + href
                                else:
                                    full_href = href
                                
                                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ–‡ç« 
                                if self.is_valid_article(href, title, full_href):
                                    article_link = full_href
                                    article_title = title
                                    break
                        
                        if article_link and article_title:
                            # åœ¨åŒä¸€ä¸ªå®¹å™¨å†…æŸ¥æ‰¾æ—¶é—´ä¿¡æ¯
                            date_elem = container.query_selector(".GenericItem_mck-c-generic-item__display-date__79HZa, [class*='date'], time")
                            date_text = "æœªæ‰¾åˆ°æ—¶é—´"
                            
                            if date_elem:
                                try:
                                    date_text = date_elem.inner_text().strip()
                                    if date_text and date_text.endswith(' -'):
                                        date_text = date_text[:-2].strip()
                                    if not date_text:
                                        date_text = "æœªæ‰¾åˆ°æ—¶é—´"
                                except:
                                    date_text = "è·å–å¤±è´¥"
                            
                            # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ï¼ˆé¿å…é‡å¤ï¼‰
                            if not any(art['url'] == article_link for art in matched_articles):
                                matched_articles.append({
                                    "title": article_title,
                                    "url": article_link,
                                    "date": date_text,
                                    "snippet": ""
                                })
                                logger.info(f"{len(matched_articles)}. {article_title[:50]}... - {date_text}")
                    
                    except Exception as e:
                        logger.error(f"å¤„ç†å®¹å™¨ {i+1} å¤±è´¥: {e}")
                        continue
                
                # å¦‚æœåŒ¹é…çš„æ–‡ç« æ•°é‡ä¸å¤Ÿï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•ä½œä¸ºè¡¥å……
                if len(matched_articles) < len(all_articles):
                    logger.info(f"åŒ¹é…æ–‡ç« æ•° ({len(matched_articles)}) å°‘äºæ€»æ–‡ç« æ•° ({len(all_articles)})ï¼Œä½¿ç”¨è¡¥å……æ–¹æ³•...")
                    
                    # ä¸ºæœªåŒ¹é…çš„æ–‡ç« åˆ†é…å‰©ä½™æ—¶é—´
                    all_date_elements = page.query_selector_all(".GenericItem_mck-c-generic-item__display-date__79HZa")
                    remaining_dates = []
                    for date_elem in all_date_elements:
                        try:
                            date_text = date_elem.inner_text().strip()
                            if date_text and date_text.endswith(' -'):
                                date_text = date_text[:-2].strip()
                            if date_text:
                                remaining_dates.append(date_text)
                        except:
                            remaining_dates.append("è·å–å¤±è´¥")
                    
                    # è¡¥å……æœªåŒ¹é…çš„æ–‡ç« 
                    matched_urls = {art['url'] for art in matched_articles}
                    date_index = len(matched_articles)
                    
                    for article in all_articles:
                        if article['url'] not in matched_urls:
                            if date_index < len(remaining_dates):
                                article['date'] = remaining_dates[date_index]
                            else:
                                article['date'] = "æœªæ‰¾åˆ°æ—¶é—´"
                            matched_articles.append(article)
                            logger.info(f"{len(matched_articles)}. {article['title'][:50]}... - {article['date']}")
                            date_index += 1
                
                # ä½¿ç”¨åŒ¹é…åçš„æ–‡ç« åˆ—è¡¨
                all_articles = matched_articles
                logger.info(f"æœ€ç»ˆåŒ¹é…åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")
                
                # è¿‡æ»¤æ–°æ–‡ç« 
                newer_articles = []
                for article in all_articles:
                    article_date = self.parse_date_for_sorting(article['date'])
                    
                    if article['url'] not in existing_urls and article_date > latest_date:
                        # å†æ¬¡åº”ç”¨ç­›é€‰é€»è¾‘ç¡®ä¿æ˜¯çœŸæ­£çš„æ–‡ç« 
                        href = article['url'].replace('https://www.mckinsey.com', '')
                        if self.is_valid_article(href, article['title'], article['url']):
                            newer_articles.append(article)
                            logger.info(f"âœ… æ–°æ–‡ç« : {article['title'][:50]}... ({article['date']})")
                        else:
                            logger.info(f"âŒ è¿‡æ»¤éæ–‡ç« : {article['title'][:50]}... ({article['date']})")
                    else:
                        if article['url'] in existing_urls:
                            logger.info(f"â­ï¸ è·³è¿‡é‡å¤: {article['title'][:50]}...")
                        elif article_date <= latest_date:
                            logger.info(f"â­ï¸ è·³è¿‡æ—§æ–‡ç« : {article['title'][:50]}... ({article['date']})")
                
                logger.info(f"æ‰¾åˆ° {len(newer_articles)} ç¯‡æ–°æ–‡ç« ")
                
                # æŒ‰æ—¥æœŸæ’åºï¼Œè·å–æœ€æ–°çš„ä¸¤ç¯‡
                newer_articles.sort(key=lambda x: self.parse_date_for_sorting(x['date']), reverse=True)
                latest_two = newer_articles[:2]
                
                if latest_two:
                    logger.info(f"æœ¬æ¬¡æœ€æ–°çš„æ–‡ç« :")
                    for i, article in enumerate(latest_two, 1):
                        logger.info(f"{i}. {article['title']}")
                        logger.info(f"   æ—¶é—´: {article['date']}")
                        logger.info(f"   é“¾æ¥: {article['url']}")
                    
                    # å°†æ–°æ–‡ç« è¿½åŠ åˆ°å†å²è®°å½•ä¸­
                    updated_articles = all_historical_articles + latest_two
                    
                    # ä¿å­˜æ›´æ–°åçš„å®Œæ•´åˆ—è¡¨
                    result = {
                        "extraction_time": datetime.now().isoformat(),
                        "total_articles_found": len(all_articles),
                        "new_articles_found": len(latest_two),
                        "total_historical_count": len(updated_articles),
                        "latest_two_articles": updated_articles
                    }
                    
                    with open(self.links_file, "w", encoding="utf-8") as f:
                        json.dump(result, f, indent=2, ensure_ascii=False)
                    
                    logger.info("âœ… æ–°æ–‡ç« å·²è¿½åŠ åˆ°å†å²è®°å½•!")
                    logger.info(f"æ€»å†å²æ–‡ç« æ•°: {len(updated_articles)}")
                    
                else:
                    logger.info("âŒ æ²¡æœ‰æ‰¾åˆ°æ¯”ä¸Šæ¬¡æ›´æ–°çš„æ–‡ç« ")
                    result = {
                        "extraction_time": datetime.now().isoformat(),
                        "total_articles_found": len(all_articles),
                        "new_articles_found": 0,  # æ˜ç¡®æ ‡è®°ä¸º0
                        "total_historical_count": len(all_historical_articles),
                        "latest_two_articles": all_historical_articles
                    }
                    
                    with open(self.links_file, "w", encoding="utf-8") as f:
                        json.dump(result, f, indent=2, ensure_ascii=False)
                
                logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {self.links_file}")
                
                browser.close()
                return latest_two
                
            except Exception as e:
                logger.error(f"âŒ æå–å¤±è´¥: {e}")
                browser.close()
                return []

    def load_article_links(self):
        """ä»JSONæ–‡ä»¶åŠ è½½éœ€è¦æŠ“å–å†…å®¹çš„æ–‡ç« é“¾æ¥"""
        if not os.path.exists(self.links_file):
            logger.error(f"âŒ æœªæ‰¾åˆ°é“¾æ¥æ–‡ä»¶: {self.links_file}")
            return []
        
        try:
            with open(self.links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ç« 
            new_articles_count = data.get('new_articles_found', 0)
            
            if new_articles_count == 0:
                logger.info("ğŸš« æ²¡æœ‰æ–°æ–‡ç« éœ€è¦æŠ“å–ï¼Œè·³è¿‡å†…å®¹æŠ“å–æ­¥éª¤")
                return []
            
            articles = []
            if 'latest_two_articles' in data:
                all_articles = data['latest_two_articles']
                
                # åªè·å–æœ€æ–°æ·»åŠ çš„æ–‡ç« 
                if new_articles_count > 0:
                    articles = all_articles[-new_articles_count:]
                    logger.info(f"âœ… å‘ç° {new_articles_count} ç¯‡æ–°æ–‡ç« éœ€è¦æŠ“å–å†…å®¹")
            
            for i, article in enumerate(articles, 1):
                logger.info(f"{i}. {article.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                logger.info(f"   é“¾æ¥: {article.get('url', '')}")
                logger.info(f"   æ—¥æœŸ: {article.get('date', 'æœªçŸ¥æ—¥æœŸ')}")
            
            return articles
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é“¾æ¥æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def scrape_article_content(self, article_url):
        """æŠ“å–å•ç¯‡æ–‡ç« å†…å®¹"""
        with sync_playwright() as p:
            browser = p.firefox.launch(
                headless=True,
                firefox_user_prefs={
                    "dom.webdriver.enabled": False,
                    "useAutomationExtension": False,
                }
            )
            context = browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/131.0.0.0 Safari/537.36"
                ),
                viewport={"width": 1280, "height": 800},
                locale='en-US',
                timezone_id='America/New_York'
            )
            
            # è®¾ç½®é¢å¤–çš„æµè§ˆå™¨å±æ€§æ¥é¿å…è¢«æ£€æµ‹
            page = context.new_page()
            
            # éšè— webdriver ç‰¹å¾
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)

            try:
                logger.info(f"ğŸŒ è®¿é—®æ–‡ç« : {article_url}")
                
                # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 60 ç§’ï¼Œå¹¶ä½¿ç”¨é‡è¯•æœºåˆ¶
                max_retries = 3
                retry_delay = 5
                
                for attempt in range(max_retries):
                    try:
                        logger.info(f"å°è¯• {attempt + 1}/{max_retries}...")
                        page.goto(
                            article_url, 
                            wait_until="domcontentloaded",
                            timeout=60000  # 60ç§’è¶…æ—¶
                        )
                        page.wait_for_timeout(8000)
                        break  # æˆåŠŸåˆ™è·³å‡ºå¾ªç¯
                    except Exception as e:
                        if attempt < max_retries - 1:
                            logger.warning(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                            logger.info(f"â° ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                        else:
                            raise  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸
                
                logger.info(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page.title()}")

                # è·å–æ–‡ç« æ ‡é¢˜
                title_elem = page.query_selector("h1")
                if title_elem:
                    article_title = title_elem.inner_text().strip()
                    logger.info(f"ğŸ“° æ–‡ç« æ ‡é¢˜: {article_title}")
                else:
                    article_title = "æœªçŸ¥æ ‡é¢˜"
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°æ–‡ç« æ ‡é¢˜")

                # è·å–å‘å¸ƒæ—¥æœŸ
                date_elem = page.query_selector("time[datetime]")
                if date_elem:
                    article_date = date_elem.get_attribute("datetime")
                    date_text = date_elem.inner_text().strip()
                    logger.info(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {date_text} ({article_date})")
                else:
                    article_date = ""
                    date_text = ""
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°å‘å¸ƒæ—¥æœŸ")

                # è·å–æ–‡ç« å†…å®¹
                main_content = page.query_selector("[role='main']")
                content = []

                if main_content:
                    logger.info("âœ… æ‰¾åˆ°ä¸»è¦å†…å®¹å®¹å™¨")
                    
                    paragraphs = main_content.query_selector_all("p")
                    logger.info(f"ğŸ“ æ‰¾åˆ° {len(paragraphs)} ä¸ªæ®µè½")
                    
                    for i, ptag in enumerate(paragraphs):
                        text = ptag.inner_text().strip()
                        if text and len(text) > 20:
                            content.append(text)
                            if i < 2:  # æ˜¾ç¤ºå‰2ä¸ªæ®µè½çš„é¢„è§ˆ
                                logger.info(f"   æ®µè½{i+1}: {text[:100]}...")

                    logger.info(f"âœ… æœ‰æ•ˆæ®µè½æ•°: {len(content)}")

                    # ç»„è£… Markdown
                    md = f"# {article_title}\n\n"
                    if date_text:
                        md += f"**å‘å¸ƒæ—¥æœŸ**: {date_text}\n"
                    if article_date:
                        md += f"**æ—¥æœŸ**: {article_date}\n"
                    md += f"**åŸæ–‡é“¾æ¥**: {article_url}\n\n"
                    
                    md += "## æ­£æ–‡å†…å®¹\n\n"
                    for p in content:
                        md += f"{p}\n\n"

                    return {
                        "title": article_title,
                        "url": article_url,
                        "date": date_text or article_date,
                        "content": content,
                        "markdown": md,
                        "success": True,
                        "error": None
                    }

                else:
                    logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹å®¹å™¨")
                    return {
                        "title": article_title,
                        "url": article_url,
                        "date": date_text or article_date,
                        "content": [],
                        "markdown": "",
                        "success": False,
                        "error": "æœªæ‰¾åˆ°ä¸»è¦å†…å®¹"
                    }

            except Exception as e:
                logger.error(f"âŒ æŠ“å–å¤±è´¥: {e}")
                return {
                    "title": "æŠ“å–å¤±è´¥",
                    "url": article_url,
                    "date": "",
                    "content": [],
                    "markdown": "",
                    "success": False,
                    "error": str(e)
                }
            finally:
                browser.close()

    def batch_scrape_articles(self):
        """æ‰¹é‡æŠ“å–æ–‡ç« å†…å®¹"""
        logger.info("ğŸ“š æ­¥éª¤2: æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ç« éœ€è¦æŠ“å–...")
        
        # åŠ è½½æ–‡ç« é“¾æ¥
        articles = self.load_article_links()
        
        if not articles:
            logger.info("âœ… æ²¡æœ‰æ–°æ–‡ç« éœ€è¦æŠ“å–")
            return []
        
        logger.info(f"ğŸ“° å¼€å§‹æ‰¹é‡æŠ“å– {len(articles)} ç¯‡æ–°æ–‡ç« çš„å†…å®¹...")
        results = []
        
        for i, article_info in enumerate(articles, 1):
            logger.info(f"ğŸ”„ æŠ“å–ç¬¬ {i}/{len(articles)} ç¯‡æ–‡ç« ")
            
            url = article_info.get('url', '')
            if not url:
                logger.warning("â­ï¸ è·³è¿‡ï¼šæ–‡ç« é“¾æ¥ä¸ºç©º")
                continue
            
            # æŠ“å–æ–‡ç« å†…å®¹
            result = self.scrape_article_content(url)
            
            # è¡¥å……åŸå§‹ä¿¡æ¯
            result['original_info'] = article_info
            results.append(result)
            
            logger.info(f"âœ… æŠ“å–å®Œæˆ: {result['title'][:50]}...")
            logger.info(f"   æˆåŠŸ: {'æ˜¯' if result['success'] else 'å¦'}")
            if result['success']:
                logger.info(f"   å†…å®¹æ®µè½æ•°: {len(result['content'])}")
            else:
                logger.info(f"   é”™è¯¯: {result['error']}")
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if i < len(articles):
                logger.info("â° ç­‰å¾…3ç§’åç»§ç»­...")
                time.sleep(3)
        
        return results

    def create_zip_file(self, markdown_dir, zip_path):
        """åˆ›å»ºZIPæ–‡ä»¶"""
        try:
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                for root, dirs, files in os.walk(markdown_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        # è®¡ç®—ç›¸å¯¹è·¯å¾„
                        arcname = os.path.relpath(file_path, markdown_dir)
                        zip_file.write(file_path, arcname)
                        logger.info(f"   æ·»åŠ åˆ°ZIP: {arcname}")
            logger.info(f"âœ… ZIPæ–‡ä»¶åˆ›å»ºæˆåŠŸ: {zip_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºZIPæ–‡ä»¶å¤±è´¥: {e}")
            return False

    def save_results_for_n8n(self, results):
        """ä¿å­˜æŠ“å–ç»“æœåˆ°N8Nç›‘æ§çš„æ–‡ä»¶å¤¹"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        logger.info("ğŸ’¾ æ­¥éª¤3: ä¿å­˜æ–‡ä»¶åˆ°N8Nç›‘æ§ç›®å½•...")
        
        # 1. ä¿å­˜è¯¦ç»†çš„JSONç»“æœåˆ°N8Nç›‘æ§æ–‡ä»¶å¤¹
        output_data = {
            "extraction_time": datetime.now().isoformat(),
            "total_articles": len(results),
            "successful_articles": len([r for r in results if r['success']]),
            "failed_articles": len([r for r in results if not r['success']]),
            "articles": results,
            "n8n_metadata": {
                "trigger_type": "file_created",
                "expected_actions": ["upload_to_google_drive"],
                "file_type": "json",
                "target_folder": "JSON_Data"
            }
        }
        
        json_filename = f"mckinsey_articles_content_{timestamp}.json"
        json_filepath = os.path.join(self.n8n_output_dir, json_filename)
        
        with open(json_filepath, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜åˆ°N8Nç›‘æ§ç›®å½•: {json_filepath}")
        
        # 2. ä¸ºæ¯ç¯‡æˆåŠŸçš„æ–‡ç« ä¿å­˜å•ç‹¬çš„Markdownæ–‡ä»¶ï¼ˆä¸´æ—¶ç›®å½•ï¼‰
        temp_markdown_dir = os.path.join(self.work_dir, f"temp_mckinsey_articles_{timestamp}")
        os.makedirs(temp_markdown_dir, exist_ok=True)
        
        successful_count = 0
        for i, result in enumerate(results, 1):
            if result['success'] and result['markdown']:
                # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
                safe_title = "".join(c for c in result['title'] if c.isalnum() or c in (' ', '-', '_')).strip()
                safe_title = safe_title[:50]  # é™åˆ¶é•¿åº¦
                
                md_filename = f"{i:02d}_{safe_title}.md"
                md_filepath = os.path.join(temp_markdown_dir, md_filename)
                
                with open(md_filepath, "w", encoding="utf-8") as f:
                    f.write(result['markdown'])
                
                successful_count += 1
                logger.info(f"ğŸ“„ Markdownæ–‡ä»¶ {i}: {md_filename}")
        
        logger.info(f"âœ… æˆåŠŸåˆ›å»º {successful_count} ä¸ªMarkdownæ–‡ä»¶")
        
        # 3. å°†Markdownæ–‡ä»¶å¤¹æ‰“åŒ…æˆZIPå¹¶æ”¾åˆ°N8Nç›‘æ§ç›®å½•
        zip_created = False
        if successful_count > 0:
            zip_filename = f"mckinsey_articles_{timestamp}.zip"
            zip_filepath = os.path.join(self.n8n_output_dir, zip_filename)
            
            logger.info(f"ğŸ“¦ æ­£åœ¨åˆ›å»ºZIPæ–‡ä»¶: {zip_filename}")
            if self.create_zip_file(temp_markdown_dir, zip_filepath):
                logger.info(f"âœ… ZIPæ–‡ä»¶å·²ä¿å­˜åˆ°N8Nç›‘æ§ç›®å½•: {zip_filepath}")
                zip_created = True
            
            # æ¸…ç†ä¸´æ—¶Markdownç›®å½•
            try:
                import shutil
                shutil.rmtree(temp_markdown_dir)
                logger.info(f"ğŸ§¹ ä¸´æ—¶ç›®å½•å·²æ¸…ç†: {temp_markdown_dir}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
        
        # 4. åˆ›å»ºN8Nå¤„ç†ä¿¡æ¯æ–‡ä»¶
        n8n_info = {
            "processing_time": datetime.now().isoformat(),
            "files_created": [json_filename],
            "zip_created": [],
            "total_files": 1,
            "instructions": {
                "json_file": "Upload to Google Drive JSON_Data folder"
            }
        }
        
        if zip_created:
            n8n_info["files_created"].append(f"mckinsey_articles_{timestamp}.zip")
            n8n_info["zip_created"].append(f"mckinsey_articles_{timestamp}.zip")
            n8n_info["total_files"] = 2
            n8n_info["instructions"]["zip_file"] = "Upload to Google Drive Markdown_Archives folder"
        
        info_filepath = os.path.join(self.n8n_output_dir, f"n8n_processing_info_{timestamp}.txt")
        with open(info_filepath, "w", encoding="utf-8") as f:
            f.write("ğŸ¤– N8Nè‡ªåŠ¨å¤„ç†æ–‡ä»¶ä¿¡æ¯\n")
            f.write("=" * 40 + "\n\n")
            f.write(f"å¤„ç†æ—¶é—´: {n8n_info['processing_time']}\n")
            f.write(f"åˆ›å»ºæ–‡ä»¶æ•°: {n8n_info['total_files']}\n\n")
            f.write("æ–‡ä»¶åˆ—è¡¨:\n")
            for file in n8n_info["files_created"]:
                f.write(f"- {file}\n")
            f.write("\nå¤„ç†è¯´æ˜:\n")
            f.write("- JSONæ–‡ä»¶å°†è¢«N8Nè‡ªåŠ¨ä¸Šä¼ åˆ°Google Driveçš„JSON_Dataæ–‡ä»¶å¤¹\n")
            if zip_created:
                f.write("- ZIPæ–‡ä»¶å°†è¢«N8Nè‡ªåŠ¨ä¸Šä¼ åˆ°Google Driveçš„Markdown_Archivesæ–‡ä»¶å¤¹\n")
            f.write("- ä¸Šä¼ å®Œæˆåï¼Œæœ¬åœ°æ–‡ä»¶å¯ä»¥è¢«åˆ é™¤\n")
        
        logger.info(f"ğŸ“‹ N8Nå¤„ç†ä¿¡æ¯å·²ä¿å­˜: {info_filepath}")
        
        logger.info(f"ğŸ¯ æ–‡ä»¶å·²å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…N8Nè‡ªåŠ¨å¤„ç†...")
        logger.info(f"   ç›‘æ§ç›®å½•: {self.n8n_output_dir}")
        logger.info(f"   JSONæ–‡ä»¶: {json_filename}")
        if zip_created:
            logger.info(f"   ZIPæ–‡ä»¶: mckinsey_articles_{timestamp}.zip")
        
        return n8n_info

    def run_complete_scraping(self):
        """è¿è¡Œå®Œæ•´çš„æŠ“å–æµç¨‹"""
        logger.info("ğŸš€ McKinseyæ–‡ç« å®Œæ•´æŠ“å–ç³»ç»Ÿ - APIç‰ˆ")
        logger.info(f"N8Nç›‘æ§ç›®å½•: {self.n8n_output_dir}")
        
        # æ­¥éª¤1: æå–æœ€æ–°æ–‡ç« é“¾æ¥
        new_articles = self.extract_latest_articles()
        
        # æ­¥éª¤2: æ‰¹é‡æŠ“å–æ–‡ç« å†…å®¹ï¼ˆåªåœ¨æœ‰æ–°æ–‡ç« æ—¶æ‰§è¡Œï¼‰
        results = self.batch_scrape_articles()
        
        # æ­¥éª¤3: ä¿å­˜ç»“æœå¹¶ç”Ÿæˆæ–‡ä»¶
        n8n_info = None
        if results:
            n8n_info = self.save_results_for_n8n(results)
        
        # ç”Ÿæˆå“åº”æ•°æ®
        successful = [r for r in results if r['success']]
        failed = [r for r in results if not r['success']]
        
        response_data = {
            'success': True,
            'extraction_time': datetime.now().isoformat(),
            'new_articles_found': len(new_articles),
            'total_articles_processed': len(results),
            'successful_articles': len(successful),
            'failed_articles': len(failed),
            'articles': results,
            'n8n_info': n8n_info,
            'method': 'mckinsey_complete_scraping'
        }
        
        # ç”Ÿæˆmarkdownå†…å®¹
        if results:
            markdown_content = f"# McKinseyæ–‡ç« æŠ“å–ç»“æœ\n\n"
            markdown_content += f"**æŠ“å–æ—¶é—´**: {response_data['extraction_time']}\n"
            markdown_content += f"**æ–°æ–‡ç« æ•°**: {len(new_articles)}\n"
            markdown_content += f"**å¤„ç†æ€»æ•°**: {len(results)}\n"
            markdown_content += f"**æˆåŠŸæ•°é‡**: {len(successful)}\n"
            markdown_content += f"**å¤±è´¥æ•°é‡**: {len(failed)}\n\n"
            
            markdown_content += "## æˆåŠŸæŠ“å–çš„æ–‡ç« \n\n"
            for i, article in enumerate(successful, 1):
                markdown_content += f"### {i}. {article['title']}\n\n"
                markdown_content += f"**URL**: {article['url']}\n"
                markdown_content += f"**æ—¥æœŸ**: {article.get('date', 'æœªçŸ¥')}\n"
                markdown_content += f"**æ®µè½æ•°**: {len(article.get('content', []))}\n\n"
                # æ·»åŠ å‰ä¸¤æ®µå†…å®¹é¢„è§ˆ
                if article.get('content'):
                    markdown_content += "**å†…å®¹é¢„è§ˆ**:\n"
                    for j, para in enumerate(article['content'][:2]):
                        markdown_content += f"{para}\n\n"
                    if len(article['content']) > 2:
                        markdown_content += f"... (è¿˜æœ‰ {len(article['content'])-2} æ®µ)\n\n"
                markdown_content += "---\n\n"
            
            if failed:
                markdown_content += "## å¤±è´¥çš„æ–‡ç« \n\n"
                for i, article in enumerate(failed, 1):
                    markdown_content += f"{i}. {article.get('title', 'æœªçŸ¥æ ‡é¢˜')} - {article.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
            
            response_data['markdown'] = markdown_content
        else:
            response_data['markdown'] = "# McKinseyæ–‡ç« æŠ“å–ç»“æœ\n\næ²¡æœ‰æ–°æ–‡ç« éœ€è¦å¤„ç†ã€‚"
        
        return response_data

# åˆ›å»ºå…¨å±€çš„çˆ¬è™«å®ä¾‹
scraper = McKinseyScraperAPI()

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'mckinsey-article-scraper-api'
    })

@app.route('/scrape', methods=['GET', 'POST'])
def scrape_articles():
    """æŠ“å–McKinseyæ–‡ç« æ¥å£"""
    try:
        # è·å–è¯·æ±‚å‚æ•° - å…¼å®¹GETå’ŒPOSTè¯·æ±‚
        if request.method == 'POST':
            try:
                data = request.get_json(force=True) or {}
            except Exception:
                # å¦‚æœæ²¡æœ‰JSONæ•°æ®æˆ–Content-Typeä¸æ­£ç¡®ï¼Œä½¿ç”¨ç©ºå­—å…¸
                data = {}
        else:
            data = {}  # GETè¯·æ±‚ä¸éœ€è¦å‚æ•°
        
        logger.info("ğŸš€ æ”¶åˆ°McKinseyæ–‡ç« æŠ“å–è¯·æ±‚")
        
        # æ¸…ç†æ—§æ–‡ä»¶
        scraper.clean_old_files()
        
        # è¿è¡Œå®Œæ•´æŠ“å–æµç¨‹
        result = scraper.run_complete_scraping()
        
        logger.info(f"âœ… æŠ“å–å®Œæˆ: æ–°æ–‡ç«  {result['new_articles_found']} ç¯‡, å¤„ç† {result['total_articles_processed']} ç¯‡")
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"âŒ æŠ“å–å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/files/<path:filename>')
def serve_file(filename):
    """æä¾›æ–‡ä»¶é™æ€æœåŠ¡"""
    try:
        return send_from_directory(scraper.n8n_output_dir, filename)
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶æœåŠ¡å¤±è´¥: {e}")
        return jsonify({'error': 'File not found'}), 404

@app.route('/list', methods=['GET'])
def list_files():
    """åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶"""
    try:
        files = []
        for filename in os.listdir(scraper.n8n_output_dir):
            filepath = os.path.join(scraper.n8n_output_dir, filename)
            if os.path.isfile(filepath):
                stat = os.stat(filepath)
                files.append({
                    'filename': filename,
                    'size': stat.st_size,
                    'created_time': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    'url': f"/files/{filename}"
                })
        
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åº
        files.sort(key=lambda x: x['modified_time'], reverse=True)
        
        return jsonify({
            'success': True,
            'count': len(files),
            'files': files
        })
        
    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/clean', methods=['POST'])
def clean_old_files():
    """æ‰‹åŠ¨æ¸…ç†æ—§æ–‡ä»¶æ¥å£"""
    try:
        data = request.get_json() or {}
        max_age_hours = data.get('max_age_hours', 24)
        
        scraper.clean_old_files(max_age_hours)
        
        return jsonify({
            'success': True,
            'message': f'å·²æ¸…ç†è¶…è¿‡ {max_age_hours} å°æ—¶çš„æ—§æ–‡ä»¶',
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ æ¸…ç†æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'Not found'}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'Internal server error'}), 500

def run_server(host='0.0.0.0', port=8002, debug=False):
    """å¯åŠ¨FlaskæœåŠ¡å™¨"""
    print("McKinseyæ–‡ç« æŠ“å–APIæœåŠ¡å™¨ - Zeaburéƒ¨ç½²ç‰ˆ")
    print("=" * 50)
    print(f"æœåŠ¡åœ°å€: http://{host}:{port}")
    print("=" * 50)
    print("APIæ¥å£:")
    print(f"  POST /scrape           - æŠ“å–McKinseyæ–‡ç« ")
    print(f"  GET  /health           - å¥åº·æ£€æŸ¥")
    print("=" * 50)
    print("n8nè°ƒç”¨ç¤ºä¾‹:")
    print("  POST http://localhost:8002/scrape")
    print("  Body: {} (ç©ºå¯¹è±¡å³å¯)")
    print("=" * 50)
    
    if debug:
        app.run(host=host, port=port, debug=debug, threaded=True)
    else:
        # åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨waitressæœåŠ¡å™¨ï¼ˆé¿å…greenletä¾èµ–ï¼‰
        try:
            from waitress import serve
            print("ä½¿ç”¨Waitress WSGIæœåŠ¡å™¨")
            serve(app, host=host, port=port, threads=4)
        except ImportError:
            print("Waitressä¸å¯ç”¨ï¼Œä½¿ç”¨Flaskå†…ç½®æœåŠ¡å™¨")
            app.run(host=host, port=port, debug=False, threaded=True)

if __name__ == '__main__':
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    host = '0.0.0.0'
    port = int(os.environ.get('PORT', 8002))  # Zeaburä¼šè®¾ç½®PORTç¯å¢ƒå˜é‡
    debug = False
    
    for arg in sys.argv[1:]:
        if arg.startswith('--host='):
            host = arg.split('=', 1)[1]
        elif arg.startswith('--port='):
            port = int(arg.split('=', 1)[1])
        elif arg == '--debug':
            debug = True
        elif arg == '--help':
            print("ä½¿ç”¨æ–¹æ³•:")
            print("  python mckinsey_api_server_simplified.py [é€‰é¡¹]")
            print("é€‰é¡¹:")
            print("  --host=HOST     æœåŠ¡å™¨åœ°å€ (é»˜è®¤: 0.0.0.0)")
            print("  --port=PORT     ç«¯å£å· (é»˜è®¤: ä»ç¯å¢ƒå˜é‡PORTè·å–ï¼Œæˆ–8002)")
            print("  --debug         å¯ç”¨è°ƒè¯•æ¨¡å¼")
            print("  --help          æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
            sys.exit(0)
    
    # å¯åŠ¨æœåŠ¡å™¨
    run_server(host=host, port=port, debug=debug)
